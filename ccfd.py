# -*- coding: utf-8 -*-
"""ccfd

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O7x60Yo0jjKlGFNIVtnQvsRJqPYVDcrN
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,f1_score,classification_report

from google.colab import drive
drive.mount('/content/drive')

data_path = '/content/drive/My Drive/ccfd.csv'

dataset=pd.read_csv(data_path)
dataset.head()

N=dataset.shape
print(N)

dataset.isnull().sum()

dataset.info()

dataset['Contract_Type'].unique()

dataset['Contract_Type'].replace(['Fixed', 'Variable'], [1, 2], inplace=True)
dataset.head()

dataset['Gender'].unique()

dataset['Gender'].replace(['Male', 'Female', 'Other'], [1, 2,3], inplace=True)
dataset.head()

dataset['Marital_Status'].unique()

dataset['Marital_Status'].replace(['Divorced', 'Widowed', 'Married', 'Single'], [1, 2, 3, 4], inplace=True)
dataset.head()

dataset['Education_Level'].unique()

dataset['Education_Level'].replace(['Graduate', 'Post Graduate', 'High School', 'Doctorate','HighSchool'], [1, 2, 3, 4, 2], inplace=True)
dataset.head()

dataset['Employment'].unique()

dataset['Employment'].replace(['Employed', 'Unemployed', 'Self-Employed','SelfEmployed'], [1, 2, 3, 3], inplace=True)
dataset.head()

dataset['Transaction_Device_Type'].unique()

dataset['Transaction_Device_Type'].replace(['Desktop', 'Mobile', 'Tablet'], [1, 2, 3], inplace=True)
dataset.head()

dataset['Transaction_Type'].unique()

dataset['Transaction_Type'].replace(['Transfer', 'Withdrawal', 'Purchase'], [1, 2, 3], inplace=True)
dataset.head()

dataset['Transaction_Location'].unique()

dataset['Transaction_Location'].replace(['Delhi', 'Kolkata', 'Mumbai', 'Pune', 'Chennai', 'Hyderabad','Ahmedabad', 'Bangalore'], [1, 2, 3, 4, 5, 6, 7, 8], inplace=True)
dataset.head()

dataset['Card_Type'].unique()

dataset['Card_Type'].replace(['MasterCard', 'Visa'], [1, 2], inplace=True)
dataset.head()

dataset['Card_Verification_Method'].unique()

dataset['Card_Verification_Method'].replace(['Online', 'Offline'], [1, 2], inplace=True)
dataset.head()

dataset.describe()

# Check For Data Unbalance
Fraud_Status_counts = dataset['Fraud_Status'].value_counts()
imbalance_ratio = Fraud_Status_counts[0] / Fraud_Status_counts[1]
plt.bar(Fraud_Status_counts.index, Fraud_Status_counts.values)
plt.xticks(Fraud_Status_counts.index)
plt.xlabel('Fraud Status')
plt.ylabel('Number of Instances')
plt.title('Class Distribution')
plt.show()

# Print class counts and imbalance ratio
print("Fraud Status:")
print(Fraud_Status_counts)
print("Imbalance Ratio:", imbalance_ratio)

# Checking for outliers in amount feature
# Annual_Income_column = dataset['Annual_Income']
# plt.figure(figsize=(6, 4))
# plt.boxplot(Annual_Income_column, vert=False)
# plt.title('Box Plot of Annual_Income Column')
# plt.xlabel('Annual_Income')
# plt.show()
# Q1 = Annual_Income_column.quantile(0.25)
# Q3 = Annual_Income_column.quantile(0.75)
# IQR = Q3 - Q1
# lower_bound = Q1 - 1.5 * IQR
# upper_bound = Q3 + 1.5 * IQR
# outliers = Annual_Income_column[(Annual_Income_column < lower_bound) | (Annual_Income_column > upper_bound)]
# num_outliers = len(outliers)
# print("Number of Outliers:", num_outliers)
# print("Lower Bound:", lower_bound)
# print("Upper Bound:", upper_bound)

plt.figure(figsize=(30,30))
sns.heatmap(dataset.corr(),annot=True,cmap='YlGnBu')

plt.figure(figsize = (14,14))
plt.title('Credit Card Transactions features correlation plot (Pearson)')
corr = dataset.corr()
sns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap="Reds")
plt.show()

fraud=dataset[dataset['Fraud_Status']==1]
valid=dataset[dataset['Fraud_Status']==0]
print('Fraud Cases: {}'.format(len(dataset[dataset['Fraud_Status'] == 1])))
print('Valid Transactions: {}'.format(len(dataset[dataset['Fraud_Status'] == 0])))
plt.hist(dataset['Fraud_Status'], bins=10, color='blue', edgecolor='black')
plt.xlabel('Fraud or Non-Fraud')
plt.ylabel('Count')
plt.title('Histogram')

"""Feature Selection

Identifying Independent And Dependent Variables
"""

from sklearn.feature_selection import mutual_info_classif

mutual_info_classif

Y = dataset["Fraud_Status"]
X= dataset.drop(columns=['Fraud_Status'])

score = mutual_info_classif(X,Y)
score

imp_fea = pd.DataFrame(score,columns=['scores'])
imp_fea

"""Splitting the dataset into Training And Testing sets"""

Y = dataset["Fraud_Status"]
X= dataset.drop(columns=['Fraud_Status'])
print(X.shape)
print(Y.shape)
xdataset=X.values
ydataset=Y.values

xTrain, xTest, yTrain, yTest = train_test_split(xdataset, ydataset, test_size = 0.3, random_state =100)
print(xTest.shape)
print(yTest.shape)

dataset.info()

smote = SMOTE(sampling_strategy='auto', random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(xTrain, yTrain)

"""Hyperparameter Tuning"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# Define hyperparameters and their search space
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Define model
RFC = RandomForestClassifier()

# Perform Grid Search
grid_search = GridSearchCV(RFC, param_grid, cv=5)
grid_search.fit(xTrain, yTrain)

# Get best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Evaluate model with best hyperparameters
best_model = grid_search.best_estimator_
accuracy = best_model.score(xTest, yTest)
print("Accuracy with Best Hyperparameters:", accuracy)

"""Predictive Models"""

# Random Forest Classifier
RFC=RandomForestClassifier()
#RFC.fit(X_train_resampled, y_train_resampled)
RFC.fit(xTrain,yTrain)
ypred=RFC.predict(xTest)
accuracy=accuracy_score(yTest,ypred)
print(accuracy)
precision = precision_score(yTest, ypred, average='binary')
print(precision)
recall = recall_score(yTest, ypred)
print(recall)
cm = confusion_matrix(yTest, ypred)
print(cm)
f1 = f1_score(yTest, ypred)
print(f1)
cr=classification_report(yTest, ypred)
print(cr)
plt.figure(figsize=(8,8))
sns.heatmap(cm,annot=True,cmap="Blues",fmt="d")
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""Pickle conversion"""

import pickle
with open('ccfd.pkl','wb') as files:
    pickle.dump(RFC,files)